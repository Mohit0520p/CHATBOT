{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Romo - A General Knowledge Bot- TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Required Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "import pickle\n",
    "import string\n",
    "import random\n",
    "import timeit\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation - Convert CSV to JSON\n",
    "\n",
    "- Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHAT IS THE NAME OF THE HORSE-LIKE ANIMAL WITH...</td>\n",
       "      <td>ZEBRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WHAT IS THE NAME OF THE LONG SLEEP SOME ANIMAL...</td>\n",
       "      <td>HIBERNATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHAT IS THE NAME OF THE RUBBER OBJECT THAT IS ...</td>\n",
       "      <td>PUCK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WHAT IS THE NAME OF THE REMAINS OF PLANTS AND ...</td>\n",
       "      <td>FOSSILS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WHICH PRECIOUS GEM IS RED?</td>\n",
       "      <td>RUBY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WHAT IS THE NAME OF THE SEVERE HEADACHE THAT R...</td>\n",
       "      <td>MIGRAINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WHAT IS THE LAST NAME OF THE AUTHOR WHO WROTE ...</td>\n",
       "      <td>SHAKESPEARE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WHAT IS THE NAME OF A DRIED GRAPE?</td>\n",
       "      <td>RAISIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WHAT IS THE NAME OF THE COMIC STRIP CHARACTER ...</td>\n",
       "      <td>POPEYE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WHAT ANIMAL RUNS THE FASTEST?</td>\n",
       "      <td>CHEETAH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question       answer\n",
       "0  WHAT IS THE NAME OF THE HORSE-LIKE ANIMAL WITH...        ZEBRA\n",
       "1  WHAT IS THE NAME OF THE LONG SLEEP SOME ANIMAL...  HIBERNATION\n",
       "2  WHAT IS THE NAME OF THE RUBBER OBJECT THAT IS ...         PUCK\n",
       "3  WHAT IS THE NAME OF THE REMAINS OF PLANTS AND ...      FOSSILS\n",
       "4                         WHICH PRECIOUS GEM IS RED?         RUBY\n",
       "5  WHAT IS THE NAME OF THE SEVERE HEADACHE THAT R...     MIGRAINE\n",
       "6  WHAT IS THE LAST NAME OF THE AUTHOR WHO WROTE ...  SHAKESPEARE\n",
       "7                 WHAT IS THE NAME OF A DRIED GRAPE?       RAISIN\n",
       "8  WHAT IS THE NAME OF THE COMIC STRIP CHARACTER ...       POPEYE\n",
       "9                      WHAT ANIMAL RUNS THE FASTEST?      CHEETAH"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convdata = pd.read_csv('general_knowledge.csv')\n",
    "\n",
    "#show header of the dataset\n",
    "convdata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1993, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see size of data\n",
    "convdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some greethings and if any of these greetings are uttered, bot will reply one out the resoponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#greeting function\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"hello i need help\", \"good day\",\"hey\",\"i need help\", \"greetings\")\n",
    "GREETING_RESPONSES = [\"Good day, How may i of help?\", \"Hello, How can i help?\", \"hello\", \"I am glad! You are talking to me.\"]\n",
    "           \n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Text preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordnet Lemmatization \n",
    "\n",
    "lemmer = nltk.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "def RemovePunction(tokens):\n",
    "    return[t for t in tokens if t not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "# Create a stopword list from the standard list of stopwords available in nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We will train model and store then model using pickle, model is trained so in future we can simply load and start using model.\n",
    "\n",
    "For training simply preprocess text and convert questions to tfidf vectors and save vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time : \n",
      "16.9038768677147\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer_pickle_path = \"tfidf_vectorizer.pkl\"\n",
    "tfidf_matrix_pickle_path = \"tfidf_matrix_train.pkl\"\n",
    "start = timeit.default_timer()\n",
    "i=0\n",
    "sentences = []\n",
    "        \n",
    "for index, row in convdata.iterrows():\n",
    "    db_tokens = RemovePunction(nltk.word_tokenize(row['question']))\n",
    "    pos_db_tokens = [word for word,pos in pos_tag(db_tokens, tagset='universal')]\n",
    "    db_word_tokens = LemTokens(pos_db_tokens)\n",
    "                \n",
    "    db_filtered_sentence = [] \n",
    "    for dbw in db_word_tokens: \n",
    "        if dbw not in stop_words: \n",
    "            db_filtered_sentence.append(dbw)  \n",
    "                \n",
    "    db_filtered_sentence =\" \".join(db_filtered_sentence).lower()\n",
    "                \n",
    "    #Debugging Checkpoint\n",
    "    #print('TRAINING INPUT: '+db_filtered_sentence)\n",
    "                \n",
    "    sentences.append(db_filtered_sentence)\n",
    "    i +=1                \n",
    "    # ---------------------------------------------------------------------------#\n",
    "                \n",
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(sentences)\n",
    "        \n",
    "#train timing\n",
    "stop = timeit.default_timer()\n",
    "print (\"Training Time : \")\n",
    "print (stop - start) \n",
    "    \n",
    "f = open(tfidf_vectorizer_pickle_path, 'wb')\n",
    "pickle.dump(tfidf_vectorizer, f) \n",
    "f.close()\n",
    "    \n",
    "f = open(tfidf_matrix_pickle_path, 'wb')\n",
    "pickle.dump(tfidf_matrix_train, f) \n",
    "f.close() \n",
    "# ------------------------------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Tfidf vectors are numbers and if we want to see them, will see a lot of zeros \n",
    "print (type(tfidf_matrix_train))\n",
    "a = tfidf_matrix_train.todense()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Talk_To_Romo(test_set_sentence):\n",
    "    tfidf_vectorizer_pickle_path = \"tfidf_vectorizer.pkl\"\n",
    "    tfidf_matrix_pickle_path = \"tfidf_matrix_train.pkl\"\n",
    "    \n",
    "    i = 0\n",
    "    sentences = []\n",
    "    \n",
    "    # ---------------Tokenisation of user input -----------------------------#\n",
    "    \n",
    "    tokens = RemovePunction(nltk.word_tokenize(test_set_sentence))\n",
    "    pos_tokens = [word for word,pos in pos_tag(tokens, tagset='universal')]\n",
    "    \n",
    "    word_tokens = LemTokens(pos_tokens)\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)  \n",
    "    \n",
    "    filtered_sentence =\" \".join(filtered_sentence).lower()\n",
    "            \n",
    "    test_set = (filtered_sentence, \"\")\n",
    "    f = open(tfidf_vectorizer_pickle_path, 'rb')\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "    f.close()\n",
    "        \n",
    "    f = open(tfidf_matrix_pickle_path, 'rb')\n",
    "    tfidf_matrix_train = pickle.load(f)\n",
    "    \n",
    "    #For Tracing, comment to remove from print.\n",
    "    #print('USER INPUT:'+filtered_sentence)\n",
    "    \n",
    "    # -----------------------------------------------------------------------#\n",
    "                \n",
    "    #use the learnt dimension space to run TF-IDF on the query\n",
    "    tfidf_matrix_test = tfidf_vectorizer.transform(test_set)\n",
    "\n",
    "    #then run cosine similarity between the 2 tf-idfs\n",
    "    cosine = cosine_similarity(tfidf_matrix_test, tfidf_matrix_train)\n",
    "    \n",
    "    #if not in the topic trained.no similarity \n",
    "    idx= cosine.argsort()[0][-2]\n",
    "    flat =  cosine.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if (req_tfidf==0):\n",
    "        \n",
    "        not_understood = \"Apology, I do not understand. Can you rephrase?\"\n",
    "        \n",
    "        return not_understood, not_understood, 2\n",
    "        \n",
    "    else:\n",
    "        cosine = np.delete(cosine, 0)\n",
    "\n",
    "        #get the max score\n",
    "        max = cosine.max()\n",
    "        response_index = 0\n",
    "\n",
    "        #if max score is lower than < 0.34 > (we see can ask if need to rephrase.)\n",
    "        if (max <= 0.34): #Threshold B\n",
    "            \n",
    "            not_understood = \"Apology, I do not understand. Can you rephrase?\"\n",
    "            \n",
    "            return not_understood,not_understood, 2\n",
    "        else:\n",
    "                response_index = np.where(cosine == max)[0][0] \n",
    "                j = 0 \n",
    "                return convdata.iloc[response_index+1]['answer'], convdata.iloc[response_index+1]['question'], max\n",
    "                with open(json_file_path, \"r\") as sentences_file:\n",
    "                    reader = json.load(sentences_file)\n",
    "                    for row in reader:\n",
    "                        j += 1 \n",
    "                        if j == response_index: \n",
    "                            return row[\"answer\"], row[\"question\"], max\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................................................\n",
      "\u001b[1;37;40mRomo\u001b[0m: My name is Romo, a General Knowledge Bot.\n",
      "\u001b[1;37;40mRomo\u001b[0m: I will try my best to answer your query.\n",
      "\u001b[1;37;40mRomo\u001b[0m: If you want to exit, you can type < bye >.\n",
      "......................................................................................\n",
      "USER  :which is the largest city of romania\n",
      "......................................................................................\n",
      "\u001b[1;37;40mRomo\u001b[0m: Bucharest\n",
      "......................................................................................\n",
      "USER  :which is the fastest animal\n",
      "......................................................................................\n",
      "\u001b[1;37;40mRomo\u001b[0m: CHEETAH\n",
      "......................................................................................\n",
      "USER  :bye\n",
      "......................................................................................\n",
      "\u001b[1;37;40mRomo\u001b[0m: Bye! Hope that i am of help.\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "# Lets add some docorations around\n",
    "print(\"......................................................................................\")\n",
    "print('\\x1b[1;37;40m' + 'Romo'+'\\x1b[0m'+': '+ 'My name is Romo, a General Knowledge Bot.')\n",
    "print('\\x1b[1;37;40m' + 'Romo'+'\\x1b[0m'+': '+ 'I will try my best to answer your query.')\n",
    "print('\\x1b[1;37;40m' + 'Romo'+'\\x1b[0m'+': '+ 'If you want to exit, you can type < bye >.')\n",
    "while(flag==True):\n",
    "    print(\"......................................................................................\")\n",
    "    sentence = input('\\x1b[0;30;47m' +\"USER  \"+'\\x1b[0m'+\":\")\n",
    "    print(\"......................................................................................\")\n",
    "    if(sentence.lower()!='bye'):\n",
    "        if(greeting(sentence.lower())!=None):\n",
    "            print('\\x1b[1;37;40m' + 'Romo'+'\\x1b[0m'+': '+ greeting(sentence.lower()))\n",
    "        else:\n",
    "            response_primary, response_message, line_id_primary = Talk_To_Romo(sentence)\n",
    "            print('\\x1b[1;37;40m' + 'Romo'+'\\x1b[0m'+': '+response_primary)\n",
    "    else:\n",
    "        flag=False\n",
    "print('\\x1b[1;37;40m' + 'Romo'+'\\x1b[0m'+': '+\"Bye! Hope that i am of help.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions you can ask to Robo:\n",
    "\n",
    "- Which is fastest animal\n",
    "- which is the largest city of Romania\n",
    "- what are the most famous rocks are in Finland\n",
    "\n",
    "To finish the prorgam: \n",
    "simply say bye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
